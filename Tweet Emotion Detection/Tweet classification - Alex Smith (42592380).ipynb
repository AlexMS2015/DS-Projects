{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Tweet classification - Alex Smith (42592380).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tZWdaaO0MQ03"},"source":["Sources:\n","\n","https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184\n","https://towardsdatascience.com/sentiment-analysis-with-python-part-2-4f71e7bde59a"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1588043685496,"user_tz":-600,"elapsed":39850,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"id":"OPook2sMMSMT","outputId":"16f5d0f8-0e3d-43b6-cc2f-3b6dbea50ab5","colab":{"base_uri":"https://localhost:8080/","height":530}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from os.path import join\n","from google.colab import drive\n","import pickle\n","\n","drive.mount('/content/drive/')\n","\n","def load_pickle(path):\n","    with open(path, 'rb') as f:\n","        file = pickle.load(f)\n","        print ('Loaded %s..' %path)\n","        return file\n","\n","dataset_directory = '/content/drive/My Drive/Colab Notebooks/tweet-emotion-detection/language_dataset'\n","\n","emotions = ['anger', 'fear', 'joy', 'sadness']\n","\n","tweets_train = np.load(join(dataset_directory, 'text_train_tweets.npy'))\n","labels_train = np.load(join(dataset_directory, 'text_train_labels.npy'))\n","vocabulary = load_pickle(join(dataset_directory, 'text_word_to_idx.pkl'))\n","\n","tweets_val = np.load(join(dataset_directory, 'text_val_tweets.npy'))\n","labels_val = np.load(join(dataset_directory, 'text_val_labels.npy'))\n","\n","tweets_test_public = np.load(join(dataset_directory, 'text_test_public_tweets_rand.npy'))\n","\n","print(len(vocabulary))\n","idx_to_word = {i: w for w, i in vocabulary.items()}\n","for i in range(7):\n","  print(i, idx_to_word[i])\n","\n","sample = 1  ## YOU CAN TRY OUT OTHER TWEETS\n","\n","print('sample tweet, stored form:')\n","print(tweets_train[sample])\n","print(labels_train[sample])\n","\n","print('sample tweet, readable form:')\n","decode = []\n","#tweet_string\n","for i in range(50):\n","  decode.append(idx_to_word[tweets_train[sample][i]])\n","print(decode)\n","print(emotions[labels_train[sample]])\n","\n","print(\"\\nPossible words for each feature:\", len(vocabulary))\n","print(\"Training data X:\", tweets_train.shape)\n","print(\"Training data y:\", labels_train.shape)\n","print(\"Val data X:\", tweets_val.shape)\n","print(\"Val data y:\", labels_val.shape)\n","print(\"Test data X:\", tweets_test_public.shape)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n","Loaded /content/drive/My Drive/Colab Notebooks/tweet-emotion-detection/language_dataset/text_word_to_idx.pkl..\n","13978\n","0 <NULL>\n","1 <START>\n","2 <END>\n","3 it\n","4 makes\n","5 me\n","6 so\n","sample tweet, stored form:\n","[ 1 23 24 20 25 19 26 27 28  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0]\n","0\n","sample tweet, readable form:\n","['<START>', 'lol', 'adam', 'the', 'bull', 'with', 'his', 'fake', 'outrage', '<END>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>']\n","anger\n","\n","Possible words for each feature: 13978\n","Training data X: (7098, 52)\n","Training data y: (7098,)\n","Val data X: (1460, 52)\n","Val data y: (1460,)\n","Test data X: (4064, 52)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2gEQJTIXFQ7T","colab":{}},"source":["# create a test submission file with all tweets predicted as 'anger' (1)\n","y_test_predict_np = np.c_[np.array(range(1, 4065)).astype(int), np.ones(4064).astype(int)]\n","y_test_predict = pd.DataFrame(y_test_predict_np, columns=[\"ID\", \"Prediction\"])\n","y_test_predict.to_csv(join(dataset_directory, \"42592380-conv.csv\"), index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vcEnXWc3mqr6","colab_type":"text"},"source":["### Data Exploration"]},{"cell_type":"markdown","metadata":{"id":"XFCzDXxTmweY","colab_type":"text"},"source":["How many times does each emotion appear in the training set? Are the classes well balanced?\n"]},{"cell_type":"code","metadata":{"id":"MTwxAZbImsgF","colab_type":"code","outputId":"3ee48b8c-a3a9-4993-c008-c0312843ce47","executionInfo":{"status":"ok","timestamp":1587716878057,"user_tz":-600,"elapsed":2240,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":265}},"source":["emotion_nums, counts = np.unique(labels_train, return_counts=True)\n","plt.bar(emotions, counts);\n","\n","# Answer: the classes are well balanced"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"aqbD1p80nB9U","colab_type":"text"},"source":["Are the tweets for different emotions of similar length on average?"]},{"cell_type":"code","metadata":{"id":"pgbXcYQzm4Li","colab_type":"code","colab":{}},"source":["# # Count number of words per tweet. The tweets include items that are not words like <START>, <END> and <NULL>. Don't\n","# # include these in the count.\n","# tweets_train_wordcount = []\n","# for tweet in tweets_train:\n","#   word_count = 0\n","#   for vocab_idx in tweet:\n","#     word = idx_to_word[vocab_idx]\n","#     if word not in ['<START>', '<END>', '<NULL>']:\n","#       word_count += 1\n","#   tweets_train_wordcount.append(word_count)\n","# tweets_train_wordcount = np.array(tweets_train_wordcount)\n","\n","# # plot histograms of tweet word counts by emotion\n","# fig, ax = plt.subplots(2, 2, figsize=(12,8));\n","# fig.subplots_adjust(hspace=0.2, wspace=0.2);\n","\n","# for i in range(0, 4):\n","#   emotion_counts = tweets_train_wordcount[np.array(labels_train==i)]\n","#   ax[int(i/2), i - 2*int(i/2)].hist(emotion_counts);\n","#   ax[int(i/2), i - 2*int(i/2)].set_title(emotions[i]);\n","#   ax[int(i/2), i - 2*int(i/2)].axvline(x=np.mean(emotion_counts), color='red')\n","#   ax[int(i/2), i - 2*int(i/2)].legend(['Mean Word Count']);\n","#   ax[int(i/2), i - 2*int(i/2)].grid(True);\n","\n","# # Answer: yes, the distribution of tweet lengths and the mean tweet length is similar for the 4 emotions"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m-F7d75cxJWy","colab_type":"code","outputId":"3b2f9365-dff5-4c6b-effa-754509039fc8","executionInfo":{"status":"ok","timestamp":1587716879192,"user_tz":-600,"elapsed":3234,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":265}},"source":["tweets_train_wordcount\n","\n","anger = tweets_train_wordcount[np.array(labels_train==0)]\n","fear = tweets_train_wordcount[np.array(labels_train==1)]\n","joy = tweets_train_wordcount[np.array(labels_train==2)]\n","sadness = tweets_train_wordcount[np.array(labels_train==3)]\n","\n","plt.boxplot([anger, fear, joy, sadness]);\n","plt.xticks([1, 2, 3, 4], emotions);"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"uz406PPdt_0O","colab_type":"code","colab":{}},"source":["# Word counts, number of unique words, how many words for top 50% etc...\n","\n","# Slide 27 of 7-1 lec notes has a good way of plotting word freqs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lKIYq8QAFWic"},"source":["## Data pre-processing:"]},{"cell_type":"markdown","metadata":{"id":"EmMg_bt8mL2T","colab_type":"text"},"source":["Remove >2 of a letter, correct spelling, fix slang"]},{"cell_type":"code","metadata":{"id":"_KqQNFI3mPmG","colab_type":"code","colab":{}},"source":["# import re\n","\n","# def reduce_lengthening(text):\n","#     pattern = re.compile(r\"(.)\\1{2,}\")\n","#     return pattern.sub(r\"\\1\\1\", text)\n","\n","# word = \"aweeeeesomeeee\"\n","# word = reduce_lengthening(word)\n","# # print(word)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MbtYv2T4PtEX","colab_type":"code","colab":{}},"source":["emotion_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/tweet-emotion-detection/language_dataset/word_emotions.csv')\n","emotion_words = list(emotion_df['English (en)'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"szJEUZWd6sbH","colab_type":"text"},"source":["De-emoji, remove hashtags, stop words, punctuation and lemmatize the tweets:"]},{"cell_type":"code","metadata":{"id":"IrG3xiUz3bLv","colab_type":"code","outputId":"6ebd69c8-22c0-468b-a8d4-e59596f05aa9","executionInfo":{"status":"ok","timestamp":1588043751726,"user_tz":-600,"elapsed":41158,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":428}},"source":["# Sources - from the uni notes:\n","\n","!pip install emoji --upgrade\n","import emoji\n","\n","import string\n","from collections import defaultdict\n","\n","import nltk\n","nltk.download(['stopwords', 'punkt', 'wordnet', 'averaged_perceptron_tagger'])\n","from nltk import pos_tag\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords, wordnet as wn\n","\n","tag_map = defaultdict(lambda : wn.NOUN); tag_map['J'] = wn.ADJ; tag_map['V'] = wn.VERB; tag_map['R'] = wn.ADV\n","\n","def normalise_tweets(tweets):\n","  #tweet_tokenizer = TweetTokenizer(reduce_len=True)\n","  lemmatizer = WordNetLemmatizer()\n","  stop_punct = stopwords.words('english') + list(string.punctuation)\n","  \n","  normalised_tweets = []\n","  for tweet in tweets:\n","\n","    # Step 1 - the tweets are currently indexes. this loop converts them to a list of words without the '<START>', '<END>', '<NULL>'\n","    # and '<user>' words which are useless. It also uses word_tokenize which will split off hashtags - e.g. '#tag' becomes '# tag'. It\n","    # also uses the emoji package to convert emoji pictures into words as these could be valuable.\n","    tweet_words = []\n","    for vocab_idx in tweet:\n","      word = emoji.demojize(idx_to_word[vocab_idx])\n","      if word == '<END>': break\n","      if word not in ['<START>', '<NULL>', '<user>']:\n","        tweet_words += word_tokenize(word) # word_tokenize splits off the hashtags\n","    \n","    # Step 2 - for the tweets constructed in the first loop, the following loop tags them, then removes punctuation and digtits and\n","    # short words and thenfinally lemmatizes the word. It re-constructs the tweet as a string.\n","    normalised_tweet = \"\"\n","    for word, tag in pos_tag(tweet_words):\n","      if word not in stop_punct and not word.isdigit() and len(word) > 2:\n","        pos = tag_map[tag[0]]\n","        normalised_tweet = normalised_tweet + lemmatizer.lemmatize(word, pos) + ' '\n","\n","    normalised_tweets.append(normalised_tweet)\n","  return normalised_tweets\n","\n","tweets_train_normalised = normalise_tweets(tweets_train)\n","tweets_val_normalised = normalise_tweets(tweets_val)\n","tweets_test_normalised = normalise_tweets(tweets_test_public)\n","tweets_normalised = tweets_train_normalised + tweets_val_normalised\n","\n","tweets_train_normalised[1:5]"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting emoji\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n","\r\u001b[K     |███████▌                        | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 30kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 40kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.0MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=3610bcac65aa2aeaf43047fa4b1f94b53f3485ab3f79ce059f71a88de21822db\n","  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-0.5.4\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['lol adam bull fake outrage ',\n"," 'pass away early morning fast furious styled car crash leave atl strip club rough stuff ',\n"," 'lol wow gon say really haha see chris nah dont even snap anymore dude ',\n"," 'need bento_box sushi date rice_ball spaghetti olive guard date cheese_wedge oncoming_fist medium_light_skin_tone rockys date pizza ']"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"EjCRBh8WYJfm","colab_type":"text"},"source":["## Feature engineering:"]},{"cell_type":"markdown","metadata":{"id":"FYxftPo9YRhe","colab_type":"text"},"source":["Vectorise the words using a TF-IDF approach:"]},{"cell_type":"code","metadata":{"id":"TNOWz9cFYDD7","colab_type":"code","outputId":"f3521450-2e95-48c2-ccf2-5abac3bad30b","executionInfo":{"status":"ok","timestamp":1588043760124,"user_tz":-600,"elapsed":1711,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import copy\n","\n","# there are 7098 tweets in the dataset, hence we need to limit to the features to something reasonable. Start with 3500:\n","tfidf_train = TfidfVectorizer(ngram_range=(1, 2), max_features = 3500)\n","X_train = tfidf_train.fit_transform(tweets_train_normalised)\n","X_val = tfidf_train.transform(tweets_val_normalised)\n","y_train = labels_train\n","y_val = labels_val\n","\n","tfidf_all = copy.deepcopy(tfidf_train)\n","X = tfidf_all.fit_transform(tweets_normalised)\n","y = np.concatenate([y_train, y_val])\n","X_test = tfidf_all.transform(tweets_test_normalised)\n","\n","X_train.shape, y_train.shape, X_val.shape, y_val.shape, X.shape, y.shape, X_test.shape"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((7098, 3500),\n"," (7098,),\n"," (1460, 3500),\n"," (1460,),\n"," (8558, 3500),\n"," (8558,),\n"," (4064, 3500))"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8_085FNIUdN2"},"source":["## Train ML model:"]},{"cell_type":"markdown","metadata":{"id":"YNmH6kLbYzVv","colab_type":"text"},"source":["Fit a Naive Bayes classifier to give a baseline for performance measurement on other models:"]},{"cell_type":"code","metadata":{"id":"g4bvj1HOYyfp","colab_type":"code","outputId":"9b402afd-0c00-4cff-e773-05c565d044b9","executionInfo":{"status":"ok","timestamp":1588043763745,"user_tz":-600,"elapsed":1235,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.metrics import accuracy_score\n","from sklearn.naive_bayes import MultinomialNB\n","\n","nb_clf = MultinomialNB()#fit_prior=False)\n","nb_clf.fit(X_train, y_train)\n","print(accuracy_score(y_train, nb_clf.predict(X_train)), accuracy_score(y_val, nb_clf.predict(X_val)))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["0.9119470273316427 0.43424657534246575\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rLcryu8FY77d","colab_type":"text"},"source":["Try several candidate models with default settings to see which might be worth investigating further:"]},{"cell_type":"code","metadata":{"id":"Uq1hBQnKX1xE","colab_type":"code","colab":{}},"source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.svm import LinearSVC, SVC\n","\n","models = {\"Naive Bayes\" : MultinomialNB(),\n","          \"KNN\" : KNeighborsClassifier(n_neighbors = 10),\n","          \"Logistic Reg\" : LogisticRegression(multi_class='ovr', max_iter=200),\n","          \"Softmax\" : LogisticRegression(multi_class='multinomial', solver='lbfgs'),\n","          \"Linear SVM\" : LinearSVC(C=10),\n","          \"Non-Linear SVM\" : SVC(),\n","          \"Random Forest\" : RandomForestClassifier(random_state=0),\n","          \"Grad. Boosting\" : GradientBoostingClassifier()}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3XjN6xtIZDEu","colab_type":"text"},"source":["KNN does not scale well when the number of features is high relative to the number of instances so performs poorly on both training and validation set. The other models perform very well on the training set but not very well on the validation set. Perhaps they are overfitting?\n"]},{"cell_type":"code","metadata":{"id":"V0B3lILUYZzJ","colab_type":"code","outputId":"d9d562cf-a129-480d-a594-645184eeccdd","executionInfo":{"status":"ok","timestamp":1588043798367,"user_tz":-600,"elapsed":27678,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":301}},"source":["from IPython.display import display\n","\n","def run_models(models_dict, X_train, y_train, X_val, y_val):\n","  training_accuracies = []\n","  validation_accuracies = []\n","  column_titles = []\n","  for model_name, model in models_dict.items():\n","    column_titles.append(model_name)\n","    model.fit(X_train, y_train)\n","    training_accuracies.append(accuracy_score(y_train, model.predict(X_train)))\n","    validation_accuracies.append(accuracy_score(y_val, model.predict(X_val)))\n","\n","  display(pd.DataFrame(np.c_[np.round(training_accuracies, 4), np.round(validation_accuracies, 4)].transpose(),\n","                       columns=column_titles, index=[\"Training data\", \"Validation data\"]))\n","\n","run_models(models, X_train, y_train, X_val, y_val)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Naive Bayes</th>\n","      <th>KNN</th>\n","      <th>Logistic Reg</th>\n","      <th>Softmax</th>\n","      <th>Linear SVM</th>\n","      <th>Non-Linear SVM</th>\n","      <th>Random Forest</th>\n","      <th>Grad. Boosting</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Training data</th>\n","      <td>0.9119</td>\n","      <td>0.3726</td>\n","      <td>0.9310</td>\n","      <td>0.9424</td>\n","      <td>0.9704</td>\n","      <td>0.9689</td>\n","      <td>0.9720</td>\n","      <td>0.8367</td>\n","    </tr>\n","    <tr>\n","      <th>Validation data</th>\n","      <td>0.4342</td>\n","      <td>0.2836</td>\n","      <td>0.4507</td>\n","      <td>0.4486</td>\n","      <td>0.4384</td>\n","      <td>0.4473</td>\n","      <td>0.4363</td>\n","      <td>0.4205</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 Naive Bayes     KNN  ...  Random Forest  Grad. Boosting\n","Training data         0.9119  0.3726  ...         0.9720          0.8367\n","Validation data       0.4342  0.2836  ...         0.4363          0.4205\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"uwTrFjKe4YbK","colab_type":"text"},"source":["The linear support vector classifier performed the best of the candidate models so below I perform a grid search to see if the model paramaters (and pre-processing paramaters) can be tweaked to reduce overfitting.\n","\n","Result: the scores for the 9 models on each of the 5 folds are high for the first 4 folds (>80%) accuracy but only approx 52-53% on the 5th fold. The 5th fold contains only data from the validation set so it seems the low accuracy on the validation set for the initial candidate models (see above cell) is due to reasons other than overfitting.\n","\n","**NOTE: YOU NEED TO SHUFFLE THE DATA BEFORE DOING THIS BECAUSE IT IS ORDERED BY EMOTION!!!**"]},{"cell_type":"code","metadata":{"id":"qglL757i4cyY","colab_type":"code","outputId":"721cef29-2f29-40d4-885b-5844c9b08c53","executionInfo":{"status":"ok","timestamp":1587859266691,"user_tz":-600,"elapsed":28346,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":598}},"source":["from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import cross_val_score, GridSearchCV\n","\n","linearsvm_pipeline = Pipeline([\n","        ('tfidf', TfidfVectorizer()),\n","        ('svc', LinearSVC())\n","])\n","\n","linearsvm_grid = [{\n","    'tfidf__ngram_range':[(1,1), (1,2)],\n","    'tfidf__max_features':[500, 1000, 3500],\n","    'svc__C':[0.1, 1, 10]\n","}]\n","\n","grid_search = GridSearchCV(linearsvm_pipeline, linearsvm_grid, cv=5, scoring='accuracy')\n","grid_search.fit(tweets_normalised, y)\n","print(\"Best model based on CV scores:\", grid_search.best_params_)\n","\n","print(\"\\n Mean accuracy for 9 different paramater combinations\")\n","scores = grid_search.cv_results_\n","for mean_accuracy, params in zip(scores[\"mean_test_score\"], scores[\"params\"]):\n","   print(round(mean_accuracy, 2), params)\n","\n","print(\"\\n\")\n","for i in range(0,5):\n","  print(\"Scores for fold\", i + 1, \"=\", np.round(scores[\"split\" + str(i) + \"_test_score\"], 2))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Best model based on CV scores: {'svc__C': 0.1, 'tfidf__max_features': 3500, 'tfidf__ngram_range': (1, 1)}\n","\n"," Mean accuracy for 9 different paramater combinations\n","0.71 {'svc__C': 0.1, 'tfidf__max_features': 500, 'tfidf__ngram_range': (1, 1)}\n","0.7 {'svc__C': 0.1, 'tfidf__max_features': 500, 'tfidf__ngram_range': (1, 2)}\n","0.76 {'svc__C': 0.1, 'tfidf__max_features': 1000, 'tfidf__ngram_range': (1, 1)}\n","0.76 {'svc__C': 0.1, 'tfidf__max_features': 1000, 'tfidf__ngram_range': (1, 2)}\n","0.78 {'svc__C': 0.1, 'tfidf__max_features': 3500, 'tfidf__ngram_range': (1, 1)}\n","0.78 {'svc__C': 0.1, 'tfidf__max_features': 3500, 'tfidf__ngram_range': (1, 2)}\n","0.7 {'svc__C': 1, 'tfidf__max_features': 500, 'tfidf__ngram_range': (1, 1)}\n","0.7 {'svc__C': 1, 'tfidf__max_features': 500, 'tfidf__ngram_range': (1, 2)}\n","0.75 {'svc__C': 1, 'tfidf__max_features': 1000, 'tfidf__ngram_range': (1, 1)}\n","0.75 {'svc__C': 1, 'tfidf__max_features': 1000, 'tfidf__ngram_range': (1, 2)}\n","0.78 {'svc__C': 1, 'tfidf__max_features': 3500, 'tfidf__ngram_range': (1, 1)}\n","0.77 {'svc__C': 1, 'tfidf__max_features': 3500, 'tfidf__ngram_range': (1, 2)}\n","0.7 {'svc__C': 10, 'tfidf__max_features': 500, 'tfidf__ngram_range': (1, 1)}\n","0.69 {'svc__C': 10, 'tfidf__max_features': 500, 'tfidf__ngram_range': (1, 2)}\n","0.73 {'svc__C': 10, 'tfidf__max_features': 1000, 'tfidf__ngram_range': (1, 1)}\n","0.73 {'svc__C': 10, 'tfidf__max_features': 1000, 'tfidf__ngram_range': (1, 2)}\n","0.73 {'svc__C': 10, 'tfidf__max_features': 3500, 'tfidf__ngram_range': (1, 1)}\n","0.73 {'svc__C': 10, 'tfidf__max_features': 3500, 'tfidf__ngram_range': (1, 2)}\n","\n","\n","Scores for fold 1 = [0.75 0.74 0.81 0.81 0.84 0.84 0.75 0.74 0.82 0.8  0.84 0.84 0.74 0.74\n"," 0.79 0.78 0.79 0.79]\n","Scores for fold 2 = [0.79 0.77 0.84 0.83 0.86 0.86 0.78 0.77 0.82 0.82 0.85 0.85 0.77 0.76\n"," 0.8  0.8  0.81 0.8 ]\n","Scores for fold 3 = [0.76 0.76 0.83 0.82 0.85 0.84 0.76 0.76 0.81 0.8  0.83 0.84 0.75 0.75\n"," 0.79 0.79 0.77 0.79]\n","Scores for fold 4 = [0.75 0.74 0.81 0.81 0.84 0.84 0.75 0.74 0.8  0.79 0.85 0.83 0.75 0.74\n"," 0.78 0.78 0.8  0.79]\n","Scores for fold 5 = [0.49 0.49 0.51 0.51 0.52 0.52 0.49 0.49 0.51 0.51 0.51 0.51 0.49 0.48\n"," 0.5  0.49 0.5  0.49]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wN2EA3vLrsTJ","colab_type":"text"},"source":["Kaggle returns a score of 60% for the linear svm from above on the test data. The model is trained on the full data (training + validation) so is better than the model on the training set which only achieved 46% accuracy on the validation set:"]},{"cell_type":"code","metadata":{"id":"xQ7eCLLZrsoj","colab_type":"code","outputId":"194c8050-5ca4-49ca-d94a-06487d0667e7","executionInfo":{"status":"ok","timestamp":1587859303189,"user_tz":-600,"elapsed":1211,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def create_submission_file(model, X, y, X_test):\n","  model.fit(X, y)\n","  y_test_predict_np = np.c_[np.array(range(1, 4065)).astype(int), model.predict(X_test)]\n","  y_test_predict = pd.DataFrame(y_test_predict_np, columns=[\"ID\", \"Prediction\"])\n","  y_test_predict.to_csv(join(dataset_directory, \"42592380-conv.csv\"), index=False)\n","  return accuracy_score(y, model.predict(X))\n","\n","create_submission_file(models['Linear SVM'], X, y, X_test)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.897172236503856"]},"metadata":{"tags":[]},"execution_count":92}]},{"cell_type":"markdown","metadata":{"id":"IDXE6Z9VePR8","colab_type":"text"},"source":["Perform error analysis on the validation set using a selection of the candidate models to see what is going wrong with the predictions on the validation set:\n","\n","- All the models have higher accuracy on fear and joy than they do on anger and sadness. Some achieve >60% accuracy on fear at the expense of accuracy on other classes.\n","- All models have low accuracy (30-40%) on sadness\n","- In general; angry, fearful and sad tweets are classified as one another quite often but not often classified as joy. In particular, anger and sadness are often classified as fear.\n","- Joyful tweets are classified as fearful ~25% of the time.\n","\n","INSERT comment on how the models take very diverse approached but make the SAME ERRORS! No point using a voting classifier."]},{"cell_type":"code","metadata":{"id":"QIzkqbFGbiBo","colab_type":"code","outputId":"1431fba2-92d9-43df-81ee-d0d761db92a9","executionInfo":{"status":"ok","timestamp":1587870424926,"user_tz":-600,"elapsed":1286,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from sklearn.metrics import confusion_matrix\n","\n","for model_name in ['Naive Bayes', 'Logistic Reg', 'Linear SVM', 'Random Forest']:\n","  model = models[model_name]\n","  y_val_predict = model.predict(X_val)\n","  cmtrx = confusion_matrix(y_val, y_val_predict)\n","\n","  cmtrx = cmtrx / cmtrx.sum(axis=1, keepdims=True)\n","  df = pd.DataFrame(np.round(cmtrx,2), columns=[[model_name] * 4, [\"Predicted\"] * 4, emotions], \n","                       index=[[\"Actual\"] * 4, emotions])\n","  display(df)\n","  print(\"\\n\")"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th></th>\n","      <th colspan=\"4\" halign=\"left\">Naive Bayes</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th></th>\n","      <th colspan=\"4\" halign=\"left\">Predicted</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th></th>\n","      <th>anger</th>\n","      <th>fear</th>\n","      <th>joy</th>\n","      <th>sadness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"4\" valign=\"top\">Actual</th>\n","      <th>anger</th>\n","      <td>0.35</td>\n","      <td>0.35</td>\n","      <td>0.13</td>\n","      <td>0.17</td>\n","    </tr>\n","    <tr>\n","      <th>fear</th>\n","      <td>0.18</td>\n","      <td>0.51</td>\n","      <td>0.12</td>\n","      <td>0.18</td>\n","    </tr>\n","    <tr>\n","      <th>joy</th>\n","      <td>0.07</td>\n","      <td>0.26</td>\n","      <td>0.59</td>\n","      <td>0.09</td>\n","    </tr>\n","    <tr>\n","      <th>sadness</th>\n","      <td>0.21</td>\n","      <td>0.38</td>\n","      <td>0.14</td>\n","      <td>0.28</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Naive Bayes                    \n","                 Predicted                    \n","                     anger  fear   joy sadness\n","Actual anger          0.35  0.35  0.13    0.17\n","       fear           0.18  0.51  0.12    0.18\n","       joy            0.07  0.26  0.59    0.09\n","       sadness        0.21  0.38  0.14    0.28"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th></th>\n","      <th colspan=\"4\" halign=\"left\">Logistic Reg</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th></th>\n","      <th colspan=\"4\" halign=\"left\">Predicted</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th></th>\n","      <th>anger</th>\n","      <th>fear</th>\n","      <th>joy</th>\n","      <th>sadness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"4\" valign=\"top\">Actual</th>\n","      <th>anger</th>\n","      <td>0.33</td>\n","      <td>0.40</td>\n","      <td>0.09</td>\n","      <td>0.18</td>\n","    </tr>\n","    <tr>\n","      <th>fear</th>\n","      <td>0.16</td>\n","      <td>0.58</td>\n","      <td>0.09</td>\n","      <td>0.17</td>\n","    </tr>\n","    <tr>\n","      <th>joy</th>\n","      <td>0.09</td>\n","      <td>0.30</td>\n","      <td>0.55</td>\n","      <td>0.07</td>\n","    </tr>\n","    <tr>\n","      <th>sadness</th>\n","      <td>0.18</td>\n","      <td>0.42</td>\n","      <td>0.11</td>\n","      <td>0.29</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Logistic Reg                    \n","                  Predicted                    \n","                      anger  fear   joy sadness\n","Actual anger           0.33  0.40  0.09    0.18\n","       fear            0.16  0.58  0.09    0.17\n","       joy             0.09  0.30  0.55    0.07\n","       sadness         0.18  0.42  0.11    0.29"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th></th>\n","      <th colspan=\"4\" halign=\"left\">Linear SVM</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th></th>\n","      <th colspan=\"4\" halign=\"left\">Predicted</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th></th>\n","      <th>anger</th>\n","      <th>fear</th>\n","      <th>joy</th>\n","      <th>sadness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"4\" valign=\"top\">Actual</th>\n","      <th>anger</th>\n","      <td>0.33</td>\n","      <td>0.35</td>\n","      <td>0.11</td>\n","      <td>0.21</td>\n","    </tr>\n","    <tr>\n","      <th>fear</th>\n","      <td>0.16</td>\n","      <td>0.50</td>\n","      <td>0.11</td>\n","      <td>0.23</td>\n","    </tr>\n","    <tr>\n","      <th>joy</th>\n","      <td>0.12</td>\n","      <td>0.22</td>\n","      <td>0.55</td>\n","      <td>0.11</td>\n","    </tr>\n","    <tr>\n","      <th>sadness</th>\n","      <td>0.17</td>\n","      <td>0.36</td>\n","      <td>0.13</td>\n","      <td>0.34</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Linear SVM                    \n","                Predicted                    \n","                    anger  fear   joy sadness\n","Actual anger         0.33  0.35  0.11    0.21\n","       fear          0.16  0.50  0.11    0.23\n","       joy           0.12  0.22  0.55    0.11\n","       sadness       0.17  0.36  0.13    0.34"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th></th>\n","      <th colspan=\"4\" halign=\"left\">Random Forest</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th></th>\n","      <th colspan=\"4\" halign=\"left\">Predicted</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th></th>\n","      <th>anger</th>\n","      <th>fear</th>\n","      <th>joy</th>\n","      <th>sadness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"4\" valign=\"top\">Actual</th>\n","      <th>anger</th>\n","      <td>0.33</td>\n","      <td>0.39</td>\n","      <td>0.10</td>\n","      <td>0.18</td>\n","    </tr>\n","    <tr>\n","      <th>fear</th>\n","      <td>0.16</td>\n","      <td>0.55</td>\n","      <td>0.10</td>\n","      <td>0.19</td>\n","    </tr>\n","    <tr>\n","      <th>joy</th>\n","      <td>0.12</td>\n","      <td>0.28</td>\n","      <td>0.53</td>\n","      <td>0.07</td>\n","    </tr>\n","    <tr>\n","      <th>sadness</th>\n","      <td>0.18</td>\n","      <td>0.40</td>\n","      <td>0.13</td>\n","      <td>0.29</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Random Forest                    \n","                   Predicted                    \n","                       anger  fear   joy sadness\n","Actual anger            0.33  0.39  0.10    0.18\n","       fear             0.16  0.55  0.10    0.19\n","       joy              0.12  0.28  0.53    0.07\n","       sadness          0.18  0.40  0.13    0.29"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vOkhzAKVr3cj","colab_type":"text"},"source":["Aside from KNN, the models performed similarly on the validation test. Logistic Regression can output the feature weights so the below displays the 10 most informative features for each emotion. Some of the results make sense but some are unexplainable (e.g. 'back' is the most important feature in fearful tweets):"]},{"cell_type":"code","metadata":{"id":"DzUPGpwdId7x","colab_type":"code","outputId":"5a0df15a-4239-4e70-f863-8d0645bfd91b","executionInfo":{"status":"ok","timestamp":1588043819676,"user_tz":-600,"elapsed":1911,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Put source of where you got this idea\n","# ALSO maybe show the 10 smallest?\n","\n","feature_names = np.array(tfidf_train.get_feature_names()).reshape(-1,1)\n","\n","# make sure the model is the original one\n","lr_clf = models['Logistic Reg']\n","lr_clf.fit(X_train, y_train)\n","weights = np.round(lr_clf.coef_.transpose(), 3)\n","\n","feature_importances = pd.DataFrame(weights, columns=emotions)\n","feature_importances.insert(0, 'Feature', feature_names)\n","\n","for emotion in emotions:\n","  display(feature_importances.nlargest(10, emotion)[['Feature', emotion]])\n","  print(\"\\n\")"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Feature</th>\n","      <th>anger</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2132</th>\n","      <td>offend</td>\n","      <td>6.193</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>angry</td>\n","      <td>5.804</td>\n","    </tr>\n","    <tr>\n","      <th>89</th>\n","      <td>anger</td>\n","      <td>5.786</td>\n","    </tr>\n","    <tr>\n","      <th>2455</th>\n","      <td>rage</td>\n","      <td>5.619</td>\n","    </tr>\n","    <tr>\n","      <th>267</th>\n","      <td>bitter</td>\n","      <td>5.587</td>\n","    </tr>\n","    <tr>\n","      <th>2556</th>\n","      <td>revenge</td>\n","      <td>5.453</td>\n","    </tr>\n","    <tr>\n","      <th>1119</th>\n","      <td>fume</td>\n","      <td>5.076</td>\n","    </tr>\n","    <tr>\n","      <th>2185</th>\n","      <td>outrage</td>\n","      <td>4.816</td>\n","    </tr>\n","    <tr>\n","      <th>2930</th>\n","      <td>sting</td>\n","      <td>4.705</td>\n","    </tr>\n","    <tr>\n","      <th>2826</th>\n","      <td>snap</td>\n","      <td>4.544</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Feature  anger\n","2132   offend  6.193\n","95      angry  5.804\n","89      anger  5.786\n","2455     rage  5.619\n","267    bitter  5.587\n","2556  revenge  5.453\n","1119     fume  5.076\n","2185  outrage  4.816\n","2930    sting  4.705\n","2826     snap  4.544"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Feature</th>\n","      <th>fear</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>993</th>\n","      <td>fear</td>\n","      <td>5.211</td>\n","    </tr>\n","    <tr>\n","      <th>2025</th>\n","      <td>nervous</td>\n","      <td>5.182</td>\n","    </tr>\n","    <tr>\n","      <th>185</th>\n","      <td>awful</td>\n","      <td>5.173</td>\n","    </tr>\n","    <tr>\n","      <th>2062</th>\n","      <td>nightmare</td>\n","      <td>5.088</td>\n","    </tr>\n","    <tr>\n","      <th>2727</th>\n","      <td>shake</td>\n","      <td>5.020</td>\n","    </tr>\n","    <tr>\n","      <th>356</th>\n","      <td>bully</td>\n","      <td>4.945</td>\n","    </tr>\n","    <tr>\n","      <th>3050</th>\n","      <td>terrible</td>\n","      <td>4.901</td>\n","    </tr>\n","    <tr>\n","      <th>3057</th>\n","      <td>terrorism</td>\n","      <td>4.873</td>\n","    </tr>\n","    <tr>\n","      <th>1450</th>\n","      <td>horrible</td>\n","      <td>4.814</td>\n","    </tr>\n","    <tr>\n","      <th>1454</th>\n","      <td>horror</td>\n","      <td>4.753</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Feature   fear\n","993        fear  5.211\n","2025    nervous  5.182\n","185       awful  5.173\n","2062  nightmare  5.088\n","2727      shake  5.020\n","356       bully  4.945\n","3050   terrible  4.901\n","3057  terrorism  4.873\n","1450   horrible  4.814\n","1454     horror  4.753"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Feature</th>\n","      <th>joy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1421</th>\n","      <td>hilarious</td>\n","      <td>6.018</td>\n","    </tr>\n","    <tr>\n","      <th>2515</th>\n","      <td>rejoice</td>\n","      <td>5.938</td>\n","    </tr>\n","    <tr>\n","      <th>2811</th>\n","      <td>smile</td>\n","      <td>5.809</td>\n","    </tr>\n","    <tr>\n","      <th>2166</th>\n","      <td>optimism</td>\n","      <td>5.801</td>\n","    </tr>\n","    <tr>\n","      <th>455</th>\n","      <td>cheer</td>\n","      <td>5.774</td>\n","    </tr>\n","    <tr>\n","      <th>1344</th>\n","      <td>happy</td>\n","      <td>5.135</td>\n","    </tr>\n","    <tr>\n","      <th>1669</th>\n","      <td>laughter</td>\n","      <td>5.126</td>\n","    </tr>\n","    <tr>\n","      <th>1194</th>\n","      <td>glee</td>\n","      <td>5.082</td>\n","    </tr>\n","    <tr>\n","      <th>1744</th>\n","      <td>lively</td>\n","      <td>4.782</td>\n","    </tr>\n","    <tr>\n","      <th>2315</th>\n","      <td>playful</td>\n","      <td>4.417</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Feature    joy\n","1421  hilarious  6.018\n","2515    rejoice  5.938\n","2811      smile  5.809\n","2166   optimism  5.801\n","455       cheer  5.774\n","1344      happy  5.135\n","1669   laughter  5.126\n","1194       glee  5.082\n","1744     lively  4.782\n","2315    playful  4.417"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Feature</th>\n","      <th>sadness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>287</th>\n","      <td>blue</td>\n","      <td>6.317</td>\n","    </tr>\n","    <tr>\n","      <th>707</th>\n","      <td>depression</td>\n","      <td>5.925</td>\n","    </tr>\n","    <tr>\n","      <th>2833</th>\n","      <td>sober</td>\n","      <td>5.883</td>\n","    </tr>\n","    <tr>\n","      <th>704</th>\n","      <td>depress</td>\n","      <td>5.717</td>\n","    </tr>\n","    <tr>\n","      <th>2616</th>\n","      <td>sadness</td>\n","      <td>5.644</td>\n","    </tr>\n","    <tr>\n","      <th>3261</th>\n","      <td>unhappy</td>\n","      <td>5.500</td>\n","    </tr>\n","    <tr>\n","      <th>2611</th>\n","      <td>sad</td>\n","      <td>5.412</td>\n","    </tr>\n","    <tr>\n","      <th>657</th>\n","      <td>dark</td>\n","      <td>5.379</td>\n","    </tr>\n","    <tr>\n","      <th>2784</th>\n","      <td>sink</td>\n","      <td>5.247</td>\n","    </tr>\n","    <tr>\n","      <th>2712</th>\n","      <td>serious</td>\n","      <td>5.111</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Feature  sadness\n","287         blue    6.317\n","707   depression    5.925\n","2833       sober    5.883\n","704      depress    5.717\n","2616     sadness    5.644\n","3261     unhappy    5.500\n","2611         sad    5.412\n","657         dark    5.379\n","2784        sink    5.247\n","2712     serious    5.111"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c1SYX24f3Zp8","colab_type":"text"},"source":["## Feature selection and representation:"]},{"cell_type":"markdown","metadata":{"id":"og5TLfhQ4ASm","colab_type":"text"},"source":["The models are unlikely to be overfitting due to too much flexibility as some of them are reasonably simple models and they have all been run with default settings which are conservative (e.g. by default, the linear svc sets C=1 and logistic regression applies l2 regularisation).\n","\n","Based on the poor fit of the models on the validation (and test) sets, I think further feature engineering is required. Specifically, the number of features needs to be reduced to include only valuable ones and other methods of representing the features need to be tested."]},{"cell_type":"markdown","metadata":{"id":"hybpJWll6ak1","colab_type":"text"},"source":["*1. Delete words that don't appear very often (reduce max_features from 3500 to 1000):*\n","\n","Result: the accuracies on the training set go down which is not surprising (not by much though). Accuracies on the test set increase slightly for some models. Overall, this method of reducing features made little difference.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"p2ComC1T4A5Y","colab_type":"code","outputId":"69fe95db-39b1-4716-88df-ff00b2cdb901","executionInfo":{"status":"ok","timestamp":1587873850019,"user_tz":-600,"elapsed":22384,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":301}},"source":["tfidf_train_2 = copy.deepcopy(tfidf_train)\n","tfidf_train_2.max_features = 1000\n","X_train_2 = tfidf_train_2.fit_transform(tweets_train_normalised)\n","X_val_2 = tfidf_train_2.transform(tweets_val_normalised)\n","run_models(models, X_train_2, y_train, X_val_2, y_val)"],"execution_count":172,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Naive Bayes</th>\n","      <th>KNN</th>\n","      <th>Logistic Reg</th>\n","      <th>Softmax</th>\n","      <th>Linear SVM</th>\n","      <th>Non-Linear SVM</th>\n","      <th>Random Forest</th>\n","      <th>Grad. Boosting</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Training data</th>\n","      <td>0.8600</td>\n","      <td>0.569</td>\n","      <td>0.8860</td>\n","      <td>0.8936</td>\n","      <td>0.9287</td>\n","      <td>0.9562</td>\n","      <td>0.9672</td>\n","      <td>0.8366</td>\n","    </tr>\n","    <tr>\n","      <th>Validation data</th>\n","      <td>0.4432</td>\n","      <td>0.313</td>\n","      <td>0.4432</td>\n","      <td>0.4452</td>\n","      <td>0.4418</td>\n","      <td>0.4445</td>\n","      <td>0.4425</td>\n","      <td>0.4199</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 Naive Bayes    KNN  ...  Random Forest  Grad. Boosting\n","Training data         0.8600  0.569  ...         0.9672          0.8366\n","Validation data       0.4432  0.313  ...         0.4425          0.4199\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"uvwslR3x8t3b","colab_type":"text"},"source":["*2. Use chi-sq feature selection to keep only unigrams/bigrams most associated with the labels:*\n","\n","Result: insert..."]},{"cell_type":"code","metadata":{"id":"lpBBvHw8625i","colab_type":"code","outputId":"3f04b5b3-81aa-4343-9013-c4c85fe571d2","executionInfo":{"status":"ok","timestamp":1587873934853,"user_tz":-600,"elapsed":15484,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":145}},"source":["# https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f\n","\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","\n","tfidf_train_3 = copy.deepcopy(tfidf_train)\n","tfidf_train_3.max_features = None\n","X_train_3 = tfidf_vect.fit_transform(tweets_train_normalised)\n","X_val_3 = tfidf_vect.transform(tweets_val_normalised)\n","\n","# for emotion_num in range(4):\n","#   features_chi2 = chi2(X_val, y_val==emotion_num)\n","#   indices = np.argsort(features_chi2[0])\n","#   print(len(indices))\n","#   feature_names = np.array(tfidf_vect2.get_feature_names())[indices]\n","#   unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n","#   bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n","#   print(emotions[emotion_num])\n","#   print(unigrams[-5:])\n","#   print(bigrams[-5:], \"\\n\")\n","\n","# use k=1000 as per previous testing, the results didn't differ too much for max_features = 1000 or 3500\n","chi2_selector = SelectKBest(chi2, k=1000)\n","X_train_3 = chi2_selector.fit_transform(X_train_3, y_train)\n","X_val_3 = chi2_selector.transform(X_val_3)\n","run_models(models, X_train_3, y_train, X_val_3, y_val)"],"execution_count":173,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Naive Bayes</th>\n","      <th>KNN</th>\n","      <th>Logistic Reg</th>\n","      <th>Softmax</th>\n","      <th>Linear SVM</th>\n","      <th>Non-Linear SVM</th>\n","      <th>Random Forest</th>\n","      <th>Grad. Boosting</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Training data</th>\n","      <td>0.8966</td>\n","      <td>0.8418</td>\n","      <td>0.8983</td>\n","      <td>0.9052</td>\n","      <td>0.9383</td>\n","      <td>0.9386</td>\n","      <td>0.9639</td>\n","      <td>0.8394</td>\n","    </tr>\n","    <tr>\n","      <th>Validation data</th>\n","      <td>0.4418</td>\n","      <td>0.4363</td>\n","      <td>0.4514</td>\n","      <td>0.4500</td>\n","      <td>0.4527</td>\n","      <td>0.4486</td>\n","      <td>0.4466</td>\n","      <td>0.4212</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 Naive Bayes     KNN  ...  Random Forest  Grad. Boosting\n","Training data         0.8966  0.8418  ...         0.9639          0.8394\n","Validation data       0.4418  0.4363  ...         0.4466          0.4212\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"3Vh4ib3EkHl3","colab_type":"code","outputId":"aa1f4a53-bcb0-42a1-92f7-a17a0fcb739b","executionInfo":{"status":"ok","timestamp":1587856261397,"user_tz":-600,"elapsed":1493,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_3 = chi2_selector.fit_transform(tfidf_vect.fit_transform(tweets_normalised), y)\n","X_test_3 = chi2_selector.transform(tfidf_vect.transform(tweets_test_normalised))\n","create_submission_file(models['Linear SVM'], X_3, y, X_test_3)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8357092778686609"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"J95bNSzx_dRd","colab_type":"text"},"source":["*3. Try using a form of dimensionality reduction appropriate for a sparse matrix...*\n","\n","Result: insert..."]},{"cell_type":"code","metadata":{"id":"S25V-xbu_dtU","colab_type":"code","outputId":"cdda4435-ce29-4f7c-fe25-2394eeb7375a","executionInfo":{"status":"ok","timestamp":1587873996989,"user_tz":-600,"elapsed":13074,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":111}},"source":["from sklearn.decomposition import TruncatedSVD\n","\n","tfidf_train_4 = copy.deepcopy(tfidf_train)\n","tfidf_train_4.max_features = None\n","X_train_4 = tfidf_train_4.fit_transform(tweets_train_normalised)\n","X_val_4 = tfidf_train_4.transform(tweets_val_normalised)\n","\n","# choose to keep 1000 features for reasons previously discussed\n","tsvd = TruncatedSVD(n_components=500)\n","X_train_4 = tsvd.fit_transform(X_train_4)\n","X_val_4 = tsvd.transform(X_val_4)\n","# some of the features now contain negative values so naive bayes cannot be used\n","models_temp = {model_name: model for model_name, model in models.items() if model_name in ['Logistic Reg', 'Linear SVM']}\n","run_models(models_temp, X_train_4, y_train, X_val_4, y_val)"],"execution_count":174,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Logistic Reg</th>\n","      <th>Linear SVM</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Training data</th>\n","      <td>0.8557</td>\n","      <td>0.8888</td>\n","    </tr>\n","    <tr>\n","      <th>Validation data</th>\n","      <td>0.4438</td>\n","      <td>0.4473</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 Logistic Reg  Linear SVM\n","Training data          0.8557      0.8888\n","Validation data        0.4438      0.4473"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"XXcEhUCEwb_H","colab_type":"text"},"source":["*5. Try fit the model using only the most important features from the logistic regression*\n","\n","Result: insert..."]},{"cell_type":"code","metadata":{"id":"lHi-WtLIwca2","colab_type":"code","outputId":"6382e6ca-715b-4a40-fb18-1c437e24b302","executionInfo":{"status":"ok","timestamp":1587874079567,"user_tz":-600,"elapsed":10976,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":145}},"source":["from sklearn.feature_selection import SelectFromModel\n","\n","lr_clf = models['Logistic Reg']\n","# the model has been refit several times to need to refit on the original training data\n","lr_clf.fit(X_train, y_train)\n","\n","mifeatures = SelectFromModel(lr_clf, threshold=0, max_features=250)\n","X_train_5 = mifeatures.fit_transform(X_train, y_train)\n","X_val_5 = mifeatures.transform(X_val)\n","\n","run_models(models, X_train_5, y_train, X_val_5, y_val)"],"execution_count":175,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Naive Bayes</th>\n","      <th>KNN</th>\n","      <th>Logistic Reg</th>\n","      <th>Softmax</th>\n","      <th>Linear SVM</th>\n","      <th>Non-Linear SVM</th>\n","      <th>Random Forest</th>\n","      <th>Grad. Boosting</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Training data</th>\n","      <td>0.8552</td>\n","      <td>0.8466</td>\n","      <td>0.8676</td>\n","      <td>0.8712</td>\n","      <td>0.8817</td>\n","      <td>0.8856</td>\n","      <td>0.9204</td>\n","      <td>0.8388</td>\n","    </tr>\n","    <tr>\n","      <th>Validation data</th>\n","      <td>0.4281</td>\n","      <td>0.4425</td>\n","      <td>0.4370</td>\n","      <td>0.4363</td>\n","      <td>0.4384</td>\n","      <td>0.4342</td>\n","      <td>0.4432</td>\n","      <td>0.4219</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 Naive Bayes     KNN  ...  Random Forest  Grad. Boosting\n","Training data         0.8552  0.8466  ...         0.9204          0.8388\n","Validation data       0.4281  0.4425  ...         0.4432          0.4219\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"741c3013-9617-4c95-af2b-c4191da61deb","executionInfo":{"status":"ok","timestamp":1587860383042,"user_tz":-600,"elapsed":6657,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"id":"e8C-B985239r","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_5 = mifeatures.fit_transform(X, y)\n","X_test_5 = mifeatures.transform(X_test)\n","create_submission_file(models['Voting Clf'], X_5, y, X_test_5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8415517644309418"]},"metadata":{"tags":[]},"execution_count":107}]},{"cell_type":"markdown","metadata":{"id":"l6M39mWGFbpm","colab_type":"text"},"source":["## Use grid-search to find a better model:"]},{"cell_type":"markdown","metadata":{"id":"lAYbPq9jZY3x","colab_type":"text"},"source":["The linear support vector classifier performed the best of the candidate models so below I perform a grid search to see if the model paramaters (and pre-processing paramaters) can be tweaked to reduce overfitting.\n","\n","Result: the scores for the 9 models on each of the 5 folds are high for the first 4 folds (>80%) accuracy but only approx 52-53% on the 5th fold. The 5th fold contains only data from the validation set so it seems the low accuracy on the validation set for the initial candidate models (see above cell) is due to reasons other than overfitting."]},{"cell_type":"code","metadata":{"id":"FAugs3A6FhaL","colab_type":"code","outputId":"48438082-3367-4d04-ff32-ef02df720a0c","executionInfo":{"status":"error","timestamp":1587715258655,"user_tz":-600,"elapsed":190792,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from sklearn.preprocessing import StandardScaler\n","\n","pipeline = Pipeline([                 \n","    ('tfidf', TfidfVectorizer()),\n","    ('scaler', StandardScaler(with_mean=False)),\n","    ('chi2', SelectKBest(chi2)),\n","    ('svc', LinearSVC())\n","    #('nb', MultinomialNB())\n","])\n","\n","param_grid = [\n","    {'tfidf__use_idf':[True, False], 'tfidf__binary':[True, False], 'tfidf__ngram_range':[(1,1), (1,2)],\n","     'tfidf__max_features':[1000, 5000, None], 'scaler':[StandardScaler(with_mean=False), 'passthrough'], \n","     'chi2':['passthrough'], 'svc__C':[0.1, 1, 10]},\n","\n","    {'tfidf__use_idf':[True, False], 'tfidf__binary':[True, False], 'tfidf__ngram_range':[(1,1), (1,2)], \n","     'tfidf__max_features':[None], 'scaler':[StandardScaler(with_mean=False), 'passthrough'], \n","     'chi2__k':[250, 1000, 3000], 'svc__C':[0.1, 1, 10]},\n","]\n","\n","grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n","grid_search.fit(tweets_normalised, y)\n","print(\"Best model based on CV scores:\", grid_search.best_params_)\n","\n","print(\"\\n Mean accuracy for 9 different paramater combinations\")\n","scores = grid_search.cv_results_\n","for mean_accuracy, params in zip(scores[\"mean_test_score\"], scores[\"params\"]):\n","   print(round(mean_accuracy, 2), params)\n","\n","print(\"\\n\")\n","for i in range(0,5):\n","  print(\"Scores for fold\", i + 1, \"=\", np.round(scores[\"split\" + str(i) + \"_test_score\"], 2))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-439-bca5afa10a2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_normalised\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best model based on CV scores:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             self.loss, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"crammer_singer\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    940\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"hmKajlwTO1Vx","colab_type":"text"},"source":["As mentioned, the model performs poorly on the 5th fold (mostly comprised of the validation set) but very well on the first 4 folds in the cross-validation. Hence, the model is not overfitting (otherwise performance on all 5 folds would not be good) but there might be data mismatch. Try to confirm the issue below:\n","\n","Result: the model trained only on the validation data still only predicts 60% accuracy on the validation data. This is likely due to a lack of data."]},{"cell_type":"code","metadata":{"id":"boiYZ20fO62s","colab_type":"code","outputId":"b6f30485-96d5-415f-dc31-76198549135d","executionInfo":{"status":"ok","timestamp":1587707798923,"user_tz":-600,"elapsed":1023,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# Re-fit to see if a model trained on just the validation data can predict the validation data well:\n","\n","X_val_temp = tfidf_vect.fit_transform(tweets_val_normalised) # re-vectorize with only features from the validation data\n","X_train_temp = tfidf_vect.transform(tweets_train_normalised)\n","\n","nb_clf_temp = MultinomialNB()\n","nb_clf_temp.fit(X_val_temp, y_val)\n","print(\"Naive bayes:\", accuracy_score(y_val, nb_clf_temp.predict(X_val_temp)))\n","\n","linearsvm_clf_temp = LinearSVC()\n","linearsvm_clf_temp.fit(X_val_temp, y_val)\n","print(\"Linear SVC:\", accuracy_score(y_val, linearsvm_clf_temp.predict(X_val_temp)))\n","print(\"Linear SVC:\", accuracy_score(y_train, linearsvm_clf_temp.predict(X_train_temp)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Naive bayes: 0.5897260273972603\n","Linear SVC: 0.6068493150684932\n","Linear SVC: 0.5584671738517892\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Op1qwLAt-1Ox","colab_type":"code","colab":{}},"source":["# import itertools\n","# from nltk.collocations import BigramCollocationFinder\n","# from nltk.metrics import BigramAssocMeasures\n"," \n","# def bigram_word_feats(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n","#     bigram_finder = BigramCollocationFinder.from_words(words)\n","#     bigrams = bigram_finder.nbest(score_fn, n)\n","#     return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n"," \n","# bigram_word_feats(tweets_train_normalised[1:10])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7TqksvsoTRz8","colab_type":"text"},"source":["# More data:"]},{"cell_type":"code","metadata":{"id":"sKdxR8SwTTfg","colab_type":"code","colab":{}},"source":["# SOURCE: https://tlkh.github.io/text-emotion-classification/\n","\n","# tweets_train_new_df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/tweet-emotion-detection/language_dataset/training_new.csv\")\n","# tweets_train_new = tweets_train_new_df['content']\n","# y_new = tweets_train_new_df['Emotion']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nI31nokVYzRr","colab_type":"code","colab":{}},"source":["# def normalise_tweets_2(tweets, labels):\n","#   lemmatizer = WordNetLemmatizer()\n","#   stop_punct = stopwords.words('english') + list(string.punctuation)\n","  \n","#   normalised_tweets = []\n","#   new_labels = []\n","\n","#   index = 0\n","#   for tweet in tweets:\n","#     tweet_words = []\n","#     tweet = tweet.lower()\n","#     for word in word_tokenize(tweet):\n","#       if word not in ['<START>', '<END>', '<NULL>', '<user>'] and word in emotion_words:\n","#         tweet_words += word_tokenize(emoji.demojize(word))\n","\n","#     normalised_tweet = \"\"\n","#     for word, tag in pos_tag(tweet_words):\n","#       if word not in stop_punct and not word.isdigit():\n","#         if tag.startswith('NN'):\n","#             pos = 'n'\n","#         elif tag.startswith('VB'):\n","#             pos = 'v'\n","#         else:\n","#             pos = 'a'\n","#         normalised_tweet = normalised_tweet + lemmatizer.lemmatize(word, pos) + ' '\n","    \n","#     if len(normalised_tweet) > 0:\n","#       normalised_tweets.append(normalised_tweet)\n","#       new_labels.append(labels[index])\n","    \n","#     index += 1\n","#   return normalised_tweets, new_labels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Z99KSauZWaH","colab_type":"code","colab":{}},"source":["# tweets_train_new_normalised, y_train_new = normalise_tweets_2(tweets_train_new, y_new)\n","# tweets_train_new_normalised"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S7zWwzbrgHbf","colab_type":"code","colab":{}},"source":["# tweets_train_all = tweets_train_normalised + tweets_train_new_normalised\n","# y_train_all = np.array(list(y_train) + list(y_train_new))\n","\n","# len(tweets_train_new), len(y_new), len(tweets_train_new_normalised),len(y_train_new)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"puWt8raUZvcr","colab_type":"code","outputId":"3c12b91d-c215-4fd0-b119-259429da003a","executionInfo":{"status":"ok","timestamp":1587434220930,"user_tz":-600,"elapsed":2338,"user":{"displayName":"Alex Smith","photoUrl":"","userId":"01713317199988736550"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# # print(tweets_train_new[15])\n","# # word_tokenize(tweets_train_new[15])\n","# # normalise_tweets_2([tweets_train_new[15]])\n","\n","# tfidf_vect_2 = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n","\n","# #len(tweets_train_new_normalised[:21000]), len(tweets_train_new_normalised[21000:])\n","\n","# tfidf_vect_2.fit(tweets_train_new_normalised[:21000])\n","# X_train_new = tfidf_vect_2.transform(tweets_train_new_normalised[:21000])\n","# X_val_new = tfidf_vect_2.transform(tweets_train_new_normalised[21000:])\n","# y_train_new = y_new[:21000]\n","# y_val_new = y_new[21000:]\n","\n","# # X = tfidf_vect_2.fit_transform(tweets_normalised)\n","# # y = np.concatenate([y_train, y_val])\n","\n","# X_train_new.shape, y_train_new.shape, X_val_new.shape, y_val_new.shape#, X.shape, y.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((21000, 128696), (21000,), (5643, 128696), (5643,))"]},"metadata":{"tags":[]},"execution_count":123}]},{"cell_type":"code","metadata":{"id":"Z7akNksSdHau","colab_type":"code","colab":{}},"source":["# tweets_train_all = tweets_train_normalised + tweets_train_new_normalised\n","# y_train_new = np.array(list(y_train) + list(tweets_train_new_df['Emotion']))\n","\n","# tfidf_vect_new = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n","# tfidf_vect_new.fit(tweets_train_all)\n","# X_train_new = tfidf_vect_new.transform(tweets_train_all)\n","# X_val_new = tfidf_vect_new.transform(tweets_val_normalised)\n","\n","# X_train_new.shape, y_train_new.shape, X_val_new.shape, y_val.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O4PMJNXpS7LC","colab_type":"text"},"source":["# Other:"]},{"cell_type":"code","metadata":{"id":"ftz_MtwOdd_5","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","ada_clf = AdaBoostClassifier(\n","    DecisionTreeClassifier(max_depth=1), n_estimators=1000, learning_rate=1\n",")\n","ada_clf.fit(X_train, y_train)\n","print(accuracy_score(y_train, ada_clf.predict(X_train)), accuracy_score(y_val, ada_clf.predict(X_val)))\n","\n","from sklearn.ensemble import VotingClassifier\n","voting_clf = VotingClassifier(\n","    estimators=[('nb', nb_clf), ('svm', linearsvm_clf), ('ada', ada_clf)], voting='hard'\n",")\n","voting_clf.fit(X_train, y_train)\n","accuracy_score(y_train, voting_clf.predict(X_train)), accuracy_score(y_val, voting_clf.predict(X_val))"],"execution_count":0,"outputs":[]}]}